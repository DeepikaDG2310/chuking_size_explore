{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7707f59",
   "metadata": {},
   "source": [
    "## Exploring the ideal chunk size for RAG system usin LlamaIndex and Evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc0e8bd",
   "metadata": {},
   "source": [
    "#### Introduction:\n",
    "\n",
    "Finding an ideal chunk size that workd for your application is one of the crucial initial steps in building RAG applications. Chunk size decides from getting all required context for llm to adding noise that would deprecate the llm response quality. Hence, strking the balance here is really vital.\n",
    "\n",
    "Here, using the evaluations from LlamaIndex, we explore the different chunk size and their Faithfulness, Relavancy, and ResponseTime to determine the opt chunk size for the given application.\n",
    "\n",
    "Retrieval-augmented generation (RAG) has introduced an innovative approach that fuses the extensive retrieval capabilities of search systems with the LLM. When implementing a RAG system, one critical parameter that governs the system’s efficiency and performance is the chunk_size. How does one discern the optimal chunk size for seamless retrieval? This is where LlamaIndex Response Evaluation comes handy. In this blogpost, we'll guide you through the steps to determine the best chunk size using LlamaIndex’s Response Evaluation module. If you're unfamiliar with the Response Evaluation module, we recommend reviewing its documentation before proceeding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfdcb01",
   "metadata": {},
   "source": [
    "#### Why Chunk Size Matters\n",
    "Choosing the right chunk_size is a critical decision that can influence the efficiency and accuracy of a RAG system in several ways:\n",
    "\n",
    "**Relevance and Granularity:** A small chunk_size, like 128, yields more granular chunks. This granularity, however, presents a risk: vital information might not be among the top retrieved chunks, especially if the similarity_top_k setting is as restrictive as 2. Conversely, a chunk size of 512 is likely to encompass all necessary information within the top chunks, ensuring that answers to queries are readily available. To navigate this, we employ the Faithfulness and Relevancy metrics. These measure the absence of ‘hallucinations’ and the ‘relevancy’ of responses based on the query and the retrieved contexts respectively.\n",
    "\n",
    "**Response Generation Time:** As the chunk_size increases, so does the volume of information directed into the LLM to generate an answer. While this can ensure a more comprehensive context, it might also slow down the system. Ensuring that the added depth doesn't compromise the system's responsiveness is crucial.\n",
    "\n",
    "In essence, determining the optimal chunk_size is about striking a balance: capturing all essential information without sacrificing speed. It's vital to undergo thorough testing with various sizes to find a configuration that suits the specific use-case and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3128d07e",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Before starting with the experiment on chunk size, let's install the required libraries from requirements.txt\n",
    "\n",
    "```bash\n",
    "!pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c913263f",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be786299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from llama_index.core import (SimpleDirectoryReader,\n",
    "                                VectorStoreIndex,\n",
    "                                Settings, \n",
    "                                Document)\n",
    "\n",
    "from llama_index.core.evaluation import (FaithfulnessEvaluator,\n",
    "                                    DatasetGenerator,\n",
    "                                    RelevancyEvaluator)\n",
    "\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding \n",
    "from llama_index.embeddings.google_genai import GoogleGenAIEmbedding\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "\n",
    "import openai\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"Key present\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae21f08",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "Let's download the data from the Uber 10K SEC Filings for 2021 for this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a702ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"data/10k\", exist_ok=True)\n",
    "!curl -L https://raw.githubusercontent.com/jerryjliu/llama_index/main/docs/examples/data/10k/uber_2021.pdf -o data/10k/uber_2021.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89e76b2",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbf3b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = SimpleDirectoryReader(\"./data/10k/\")\n",
    "documents = loader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88949b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's check the number of documents loaded\n",
    "print(f\"The total number of documents loaded {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbfc1b6",
   "metadata": {},
   "source": [
    "### Generate Questions for Evaluation\n",
    "\n",
    "To select the right chunk_size, we'll compute metrics like Average Response time, Faithfulness, and Relevancy for various chunk_sizes. The DatasetGenerator will help us generate questions from the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0eb8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's generate questions to use in evaluation\n",
    "eval_documents = documents[:20]\n",
    "ques_generator = DatasetGenerator.from_documents(eval_documents)\n",
    "eval_questions = ques_generator.generate_questions_from_nodes(num=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a226360",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's preview some sample questions generated\n",
    "print(f\"Total number of questions generated {len(eval_questions)}  - {type(eval_questions)}\")\n",
    "print(\"Sample Preview of Generated Evaluation Questions:\")\n",
    "for i, question in enumerate(eval_questions[:10],1):\n",
    "    print(f\"{i}. {question}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1ce974",
   "metadata": {},
   "source": [
    "### Setting up Evaluation:\n",
    "\n",
    "In order to evaluate the RAG we use the evaluation metircs **Faithfulness** and **Relevancy**. Let's use the **GPT - 4** model for evaluation purpose, here the **GPT-4** will evaluate the response from **GPT03.5 Turbo** model.\n",
    "\n",
    "**Faithfulness** - To measure if the response is hallucinated or it is based on the ground truth(Context) provided to the model.\n",
    "\n",
    "**Relevancy** - To measure if the response is addressing the actual query also it measures if the response + source_nodes are match for the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82587b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's initialize the model and evaluation metrics\n",
    "\n",
    "Settings.llm = OpenAI(model = 'gpt-4o-mini', temperature=0.2)\n",
    "\n",
    "faithfulness = FaithfulnessEvaluator(llm=Settings.llm)\n",
    "\n",
    "relevancy = RelevancyEvaluator(llm=Settings.llm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6f91bb",
   "metadata": {},
   "source": [
    "### Function to Evaluate for chunk size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fe2049f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_response_time_accuracy(chunk_size:int,eval_queries:list)-> tuple:\n",
    "    \"\"\"Evaluate the average response time, faithfulness and relevancy for different chunk size \n",
    "    Evaluation Model : GPT-4\n",
    "    Response from the model GPT-3.5-turbo\n",
    "\n",
    "    Input Parameter:\n",
    "    chunk_size : int - The size of the data chunks\n",
    "\n",
    "    Output Parameter:\n",
    "    Tuple - average response time, faithfulness and relevancy\n",
    "    \"\"\"\n",
    "    total_time = 0\n",
    "    total_faihtfulness = 0\n",
    "    total_relevancy = 0\n",
    "\n",
    "    total_questions = len(eval_queries)\n",
    "\n",
    "    print(f\"Total number of qureries to evaluate {total_questions}\")\n",
    "\n",
    "\n",
    "    res_llm = OpenAI(model='gpt-4o-mini', temperature=0.2)\n",
    "    chunk_overlap = int(chunk_size*0.2)\n",
    "\n",
    "    Settings.text_splitter=SentenceSplitter(chunk_size = chunk_size,\n",
    "    chunk_overlap = chunk_overlap)\n",
    "\n",
    "    Settings.embed_model = OpenAIEmbedding(model='text-embedding-3-small', dimesions=1536)\n",
    "\n",
    "    vector_store = VectorStoreIndex(eval_documents,show_progress=True)\n",
    "\n",
    "    engine=vector_store.as_query_engine(similarity_top_k=5, response_mode='compact')\n",
    "\n",
    "    for question in eval_queries:\n",
    "\n",
    "        time.sleep(3)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        response = engine.query(question)\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        time.sleep(3)\n",
    "\n",
    "        faithfulness_result = faithfulness.evaluate_response(response=response)\n",
    "        # print(f\"faithfulness : {faithfulness_result.score} of type {type(faithfulness_result)}\")\n",
    "\n",
    "        time.sleep(3)\n",
    "\n",
    "        relevancy_result = relevancy.evaluate_response(query=question, response=response)\n",
    "\n",
    "        # print(f\"relevancy : {relevancy_result.score} of type :{type(relevancy_result)}\")\n",
    "\n",
    "        total_time += elapsed_time\n",
    "        total_faihtfulness += faithfulness_result.score\n",
    "        total_relevancy += relevancy_result.score\n",
    "\n",
    "    avg_response_time = total_time / total_questions\n",
    "    avg_faithfulness = total_faihtfulness / total_questions\n",
    "    avg_relevancy = total_relevancy / total_questions\n",
    "\n",
    "    return avg_response_time,avg_faithfulness,avg_relevancy\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b5c4f9",
   "metadata": {},
   "source": [
    "### Testing Across Different Chunk Sizes\n",
    "We'll evaluate a range of chunk sizes [128, 256, 512, 1024, 2048] to identify which offers the most promising metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4197f856",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_sizes = [128, 256, 512, 1024, 2048]\n",
    "eval_queries = eval_questions\n",
    "\n",
    "print(eval_queries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7a556e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for chunk_size in chunk_sizes:\n",
    "    avg_response_time, avg_faithfulness, avg_relevancy = evaluate_response_time_accuracy(chunk_size, eval_queries)\n",
    "    print(f\"The Metrics for chunk size {chunk_size} are Average reponse time {avg_response_time}, Averaage Faithfulness {avg_faithfulness} and Average Relevancy {avg_relevancy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c067c0",
   "metadata": {},
   "source": [
    "### Debugging step to see all available google genai models and their path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b08faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "\n",
    "gemi_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "client = genai.Client(api_key=gemi_key)\n",
    "\n",
    "models = client.models.list()\n",
    "\n",
    "for model in models:\n",
    "    print(model.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bbc6a90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-26 16:06:24,709 - INFO - HTTP Request: GET https://api.openai.com/v1/models \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4-0613\n",
      "gpt-4\n",
      "gpt-3.5-turbo\n",
      "gpt-4o-search-preview-2025-03-11\n",
      "gpt-5.3-codex\n",
      "gpt-realtime-1.5\n",
      "gpt-audio-1.5\n",
      "gpt-4o-search-preview\n",
      "davinci-002\n",
      "babbage-002\n",
      "gpt-3.5-turbo-instruct\n",
      "gpt-3.5-turbo-instruct-0914\n",
      "dall-e-3\n",
      "dall-e-2\n",
      "gpt-4-1106-preview\n",
      "gpt-3.5-turbo-1106\n",
      "tts-1-hd\n",
      "tts-1-1106\n",
      "tts-1-hd-1106\n",
      "text-embedding-3-small\n",
      "text-embedding-3-large\n",
      "gpt-4-0125-preview\n",
      "gpt-4-turbo-preview\n",
      "gpt-3.5-turbo-0125\n",
      "gpt-4-turbo\n",
      "gpt-4-turbo-2024-04-09\n",
      "gpt-4o\n",
      "gpt-4o-2024-05-13\n",
      "gpt-4o-mini-2024-07-18\n",
      "gpt-4o-mini\n",
      "gpt-4o-2024-08-06\n",
      "gpt-4o-audio-preview\n",
      "gpt-4o-realtime-preview\n",
      "omni-moderation-latest\n",
      "omni-moderation-2024-09-26\n",
      "gpt-4o-realtime-preview-2024-12-17\n",
      "gpt-4o-audio-preview-2024-12-17\n",
      "gpt-4o-mini-realtime-preview-2024-12-17\n",
      "gpt-4o-mini-audio-preview-2024-12-17\n",
      "o1-2024-12-17\n",
      "o1\n",
      "gpt-4o-mini-realtime-preview\n",
      "gpt-4o-mini-audio-preview\n",
      "o3-mini\n",
      "o3-mini-2025-01-31\n",
      "gpt-4o-2024-11-20\n",
      "gpt-4o-mini-search-preview-2025-03-11\n",
      "gpt-4o-mini-search-preview\n",
      "gpt-4o-transcribe\n",
      "gpt-4o-mini-transcribe\n",
      "o1-pro-2025-03-19\n",
      "o1-pro\n",
      "gpt-4o-mini-tts\n",
      "o3-2025-04-16\n",
      "o4-mini-2025-04-16\n",
      "o3\n",
      "o4-mini\n",
      "gpt-4.1-2025-04-14\n",
      "gpt-4.1\n",
      "gpt-4.1-mini-2025-04-14\n",
      "gpt-4.1-mini\n",
      "gpt-4.1-nano-2025-04-14\n",
      "gpt-4.1-nano\n",
      "gpt-image-1\n",
      "gpt-4o-realtime-preview-2025-06-03\n",
      "gpt-4o-audio-preview-2025-06-03\n",
      "gpt-4o-transcribe-diarize\n",
      "gpt-5-chat-latest\n",
      "gpt-5-2025-08-07\n",
      "gpt-5\n",
      "gpt-5-mini-2025-08-07\n",
      "gpt-5-mini\n",
      "gpt-5-nano-2025-08-07\n",
      "gpt-5-nano\n",
      "gpt-audio-2025-08-28\n",
      "gpt-realtime\n",
      "gpt-realtime-2025-08-28\n",
      "gpt-audio\n",
      "gpt-5-codex\n",
      "gpt-image-1-mini\n",
      "gpt-5-pro-2025-10-06\n",
      "gpt-5-pro\n",
      "gpt-audio-mini\n",
      "gpt-audio-mini-2025-10-06\n",
      "gpt-5-search-api\n",
      "gpt-realtime-mini\n",
      "gpt-realtime-mini-2025-10-06\n",
      "sora-2\n",
      "sora-2-pro\n",
      "gpt-5-search-api-2025-10-14\n",
      "gpt-5.1-chat-latest\n",
      "gpt-5.1-2025-11-13\n",
      "gpt-5.1\n",
      "gpt-5.1-codex\n",
      "gpt-5.1-codex-mini\n",
      "gpt-5.1-codex-max\n",
      "gpt-image-1.5\n",
      "gpt-5.2-2025-12-11\n",
      "gpt-5.2\n",
      "gpt-5.2-pro-2025-12-11\n",
      "gpt-5.2-pro\n",
      "gpt-5.2-chat-latest\n",
      "gpt-4o-mini-transcribe-2025-12-15\n",
      "gpt-4o-mini-transcribe-2025-03-20\n",
      "gpt-4o-mini-tts-2025-03-20\n",
      "gpt-4o-mini-tts-2025-12-15\n",
      "gpt-realtime-mini-2025-12-15\n",
      "gpt-audio-mini-2025-12-15\n",
      "chatgpt-image-latest\n",
      "gpt-5.2-codex\n",
      "gpt-3.5-turbo-16k\n",
      "tts-1\n",
      "whisper-1\n",
      "text-embedding-ada-002\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "client=openai.Client(api_key=openai_key)\n",
    "\n",
    "models=client.models.list()\n",
    "\n",
    "for model in models:\n",
    "    print(model.id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
